{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2852f-b12b-4c57-970c-0da57c2b4781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory where punkt is already downloaded\n",
    "nltk_data_dir = r\"D:/Data/OneDrive/Ccantu/OneDrive - CFTC/Documents/Python Scripts/punkt\"\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# Load Legal-BERT model and tokenizer\n",
    "def load_local_legal_bert():\n",
    "    model_path = r\"D:/Data/OneDrive/Ccantu/OneDrive - CFTC/Documents/Python Scripts/BERT-Legal\"\n",
    "    st.write(f\"Loading the Legal-BERT model from '{model_path}'...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    st.write(\"Legal-BERT model loaded successfully!\")\n",
    "    return tokenizer, model\n",
    "\n",
    "# Extract definitions from the text\n",
    "def extract_definitions(text):\n",
    "    definition_pattern = r\"(?P<term>\\w+)\\s+(?:is|means)\\s+(?P<definition>.*?)[;.]\" \n",
    "    definitions = {}\n",
    "    for match in re.finditer(definition_pattern, text, re.IGNORECASE):\n",
    "        term = match.group(\"term\")\n",
    "        definition = match.group(\"definition\").strip()\n",
    "        definitions[term] = definition\n",
    "    return definitions\n",
    "\n",
    "# Score sentences based on definitions\n",
    "def score_sentence(sentence, definitions, tfidf_matrix, sentence_similarities):\n",
    "    score = 0\n",
    "    for term, definition in definitions.items():\n",
    "        if term in sentence or definition in sentence: \n",
    "            score += sentence_similarities[0,1] \n",
    "    return score\n",
    "\n",
    "# Summarize the legal text\n",
    "def extractive_summarize(text, num_sentences=3):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    definitions = extract_definitions(text) \n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    sentence_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    sentence_scores = [score_sentence(sentence, definitions, tfidf_matrix, sentence_similarities) for sentence in sentences]\n",
    "    ranked_sentences = [sentences[i] for i in np.argsort(sentence_scores)[::-1]]\n",
    "    \n",
    "    summary = ' '.join(ranked_sentences[:num_sentences])\n",
    "    return summary\n",
    "\n",
    "# Process text with Legal-BERT\n",
    "def process_with_legal_bert(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Search for a term in the document\n",
    "def search_document(text, search_term):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    matches = [sentence for sentence in sentences if re.search(search_term, sentence, re.IGNORECASE)]\n",
    "    return matches\n",
    "\n",
    "# Streamlit App\n",
    "st.title(\"Legal Text Analysis with BERT\")\n",
    "\n",
    "# Load Legal-BERT\n",
    "st.write(\"Loading the Legal-BERT model...\")\n",
    "legal_bert_tokenizer, legal_bert_model = load_local_legal_bert()\n",
    "\n",
    "# Input legal text\n",
    "text = st.text_area(\"Enter the legal text:\", height=200)\n",
    "\n",
    "if st.button(\"Analyze Text\"):\n",
    "    st.write(\"**Original Text:**\")\n",
    "    st.write(text)\n",
    "\n",
    "    # Extract definitions\n",
    "    st.write(\"\\n**Definitions found:**\")\n",
    "    definitions = extract_definitions(text)\n",
    "    for term, definition in definitions.items():\n",
    "        st.write(f\"{term} means {definition}\")\n",
    "\n",
    "    # Process with Legal-BERT\n",
    "    bert_output = process_with_legal_bert(text, legal_bert_tokenizer, legal_bert_model)\n",
    "    st.write(\"\\n**Legal-BERT processing complete. Output shape:**\", bert_output.shape)\n",
    "\n",
    "    # Generate summary\n",
    "    summary = extractive_summarize(text)\n",
    "    st.write(\"\\n**Generated Summary:**\")\n",
    "    st.write(summary)\n",
    "\n",
    "# Search functionality\n",
    "search_term = st.text_input(\"Search for a term in the document:\")\n",
    "if st.button(\"Search\"):\n",
    "    matches = search_document(text, search_term)\n",
    "    st.write(f\"**Sentences containing '{search_term}':**\")\n",
    "    for match in matches:\n",
    "        st.write(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335af6bb-3070-412b-b4bb-4743f6ea549a",
   "metadata": {},
   "source": [
    "all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6ed78-be36-495f-806d-251d9e68e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory where punkt is already downloaded\n",
    "nltk_data_dir = r\"D:/Data/OneDrive/Ccantu/OneDrive - CFTC/Documents/Python Scripts/punkt\"\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# Load Legal-BERT model and tokenizer\n",
    "def load_local_legal_bert():\n",
    "    model_path = r\"D:/Data/OneDrive/Ccantu/OneDrive - CFTC/Documents/Python Scripts/BERT-Legal\"\n",
    "    st.write(f\"Loading the Legal-BERT model from '{model_path}'...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    st.write(\"Legal-BERT model loaded successfully!\")\n",
    "    return tokenizer, model\n",
    "\n",
    "# Load Fine-Tuned GPT-2 model and tokenizer\n",
    "def load_gpt2_model():\n",
    "    gpt2_model_path = r\"D:/Data/OneDrive/Ccantu/OneDrive - CFTC/Documents/Python Scripts/GPT2\"\n",
    "    st.write(f\"Loading the GPT-2 model from '{gpt2_model_path}'...\")\n",
    "    gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_path)\n",
    "    gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_model_path)\n",
    "    st.write(\"GPT-2 model loaded successfully!\")\n",
    "    return gpt2_tokenizer, gpt2_model\n",
    "\n",
    "# Simplify summary for laypeople using GPT-2\n",
    "def simplify_summary_for_layperson(summary, gpt2_model, gpt2_tokenizer):\n",
    "    input_text = f\"Simplify this legal text for a layperson: {summary}\"\n",
    "    inputs = gpt2_tokenizer(input_text, return_tensors='pt')\n",
    "    \n",
    "    outputs = gpt2_model.generate(\n",
    "        **inputs,\n",
    "        max_length=len(inputs['input_ids'][0]) + 200,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    simplified_summary = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    simplified_summary = simplified_summary.replace(input_text, \"\").strip()\n",
    "    \n",
    "    return simplified_summary\n",
    "\n",
    "# Extract definitions from the text\n",
    "def extract_definitions(text):\n",
    "    definition_pattern = r\"(?P<term>\\w+)\\s+(?:is|means)\\s+(?P<definition>.*?)[;.]\" \n",
    "    definitions = {}\n",
    "    for match in re.finditer(definition_pattern, text, re.IGNORECASE):\n",
    "        term = match.group(\"term\")\n",
    "        definition = match.group(\"definition\").strip()\n",
    "        definitions[term] = definition\n",
    "    return definitions\n",
    "\n",
    "# Score sentences based on definitions\n",
    "def score_sentence(sentence, definitions, tfidf_matrix, sentence_similarities):\n",
    "    score = 0\n",
    "    for term, definition in definitions.items():\n",
    "        if term in sentence or definition in sentence: \n",
    "            score += sentence_similarities[0,1] \n",
    "    return score\n",
    "\n",
    "# Summarize the legal text\n",
    "def extractive_summarize(text, num_sentences=3):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    definitions = extract_definitions(text) \n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    sentence_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    sentence_scores = [score_sentence(sentence, definitions, tfidf_matrix, sentence_similarities) for sentence in sentences]\n",
    "    ranked_sentences = [sentences[i] for i in np.argsort(sentence_scores)[::-1]]\n",
    "    \n",
    "    summary = ' '.join(ranked_sentences[:num_sentences])\n",
    "    return summary\n",
    "\n",
    "# Process text with Legal-BERT\n",
    "def process_with_legal_bert(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Search for a term in the document\n",
    "def search_document(text, search_term):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    matches = [sentence for sentence in sentences if re.search(search_term, sentence, re.IGNORECASE)]\n",
    "    return matches\n",
    "\n",
    "# Streamlit App\n",
    "st.title(\"Legal Text Analysis with BERT\")\n",
    "\n",
    "# Load models\n",
    "st.write(\"Loading models...\")\n",
    "legal_bert_tokenizer, legal_bert_model = load_local_legal_bert()\n",
    "gpt2_tokenizer, gpt2_model = load_gpt2_model()\n",
    "\n",
    "# Input legal text\n",
    "text = st.text_area(\"Enter the legal text:\", height=200)\n",
    "\n",
    "if st.button(\"Analyze Text\"):\n",
    "    st.write(\"**Original Text:**\")\n",
    "    st.write(text)\n",
    "\n",
    "    # Extract definitions\n",
    "    st.write(\"\\n**Definitions found:**\")\n",
    "    definitions = extract_definitions(text)\n",
    "    for term, definition in definitions.items():\n",
    "        st.write(f\"{term} means {definition}\")\n",
    "\n",
    "    # Process with Legal-BERT\n",
    "    bert_output = process_with_legal_bert(text, legal_bert_tokenizer, legal_bert_model)\n",
    "    st.write(\"\\n**Legal-BERT processing complete. Output shape:**\", bert_output.shape)\n",
    "\n",
    "    # Generate summary\n",
    "    summary = extractive_summarize(text)\n",
    "    st.write(\"\\n**Generated Summary:**\")\n",
    "    st.write(summary)\n",
    "    \n",
    "    # Simplify summary for laypeople using GPT-2\n",
    "    simplified_summary = simplify_summary_for_layperson(summary, gpt2_model, gpt2_tokenizer)\n",
    "    st.write(\"\\n**Simplified Summary for Laypeople:**\")\n",
    "    st.write(simplified_summary)\n",
    "\n",
    "# Search functionality\n",
    "search_term = st.text_input(\"Search for a term in the document:\")\n",
    "if st.button(\"Search\"):\n",
    "    matches = search_document(text, search_term)\n",
    "    st.write(f\"**Sentences containing '{search_term}':**\")\n",
    "    for match in matches:\n",
    "        st.write(match)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
